# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Decision Trees & Random Forests

> Unit 2: Flex

---

## Materials

| Topic | Description | Link |
| --- | --- | --- |
| Lesson | Decision Trees | [Here](./decision_trees.ipynb) |
| Lab | Decision Trees Lab | [Here](./practice/decision_trees-codealong.ipynb) |
| Practice | Predicting 'Evergreen' Websites | [Here](./practice/) |
| Extra | Advanced Materials | [Here](./extra-materials/) |
| Datasets | Titanic Survivors and Evergreen Websites | [Here](./datasets/) |
| Solutions | Solution code for practice | [Here](./practice/solutions/) |

> The Titanic survival dataset was chosen for this lesson because it is interesting, easy to understand, and commonly used in other tutorials. This dataset is also used in an official Kaggle tutorial (https://www.kaggle.com/c/titanic), which might be useful for students who want to explore Kaggle or the dataset further.

---

## Learning Objectives

After this lesson, students will be able to:
- **Understand** what a decision tree is and how to interpret it.
- **Implement** a decision tree construction algorithm.
- **Create** decision trees and random forests in scikit-learn.
- **Understand** how random forests improve prediction.


## Additional Resources

For more information on this topic, check out the following resources:

* [Random Forests in Python](http://blog.yhathq.com/posts/random-forests-in-python.html)
* [Random Forests for Kaggle](http://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-random-forests)
* [Kaggle Competitions won with Random Forests (scroll to bottom)](https://www.kaggle.com/wiki/RandomForests)
* [Random Forests and Performance Metrics](http://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/)
* [A video talking about Random Forests](https://www.youtube.com/watch?v=kwt6XEh7U3g#t=47m22s)
