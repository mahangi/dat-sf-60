{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Decision Trees\n",
    " \n",
    "_Author: Adam Jones, PhD (San Francisco)_\n",
    " \n",
    "---\n",
    "\n",
    "Adapted from numerous sources, including: \n",
    "- [*An Introduction to Statistical Learning*](http://www-bcf.usc.edu/~gareth/ISL/) - Chapter 8\n",
    "\n",
    "---\n",
    "\n",
    "This notebook will rely on Numpy, Matplotlib, and GraphViz.*\n",
    "\n",
    ">\\*For 'simple' installation: `$ conda install python-graphviz` \n",
    ">\n",
    ">    (Else, try `$ pip install graphviz`)\n",
    ">\n",
    ">For more info, see: https://pypi.org/project/graphviz/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id=\"learning\"></a>\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand how to use recursion in Python.\n",
    "- Observe the utility of decision trees as visualizations, capable of conveying complex, decision-like processes.\n",
    "- Identify the limitations of decision tree models, as well as some common methods for improving on them.\n",
    "- Preview some of the ‘ensembling methods’ available for decision tree models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Learning Objectives](#learning)\n",
    "- [Where Are We Now?](#where)\n",
    "- [Recursion Example](#recursion)\n",
    "- [Intuition Behind Decision Trees](#intuition)\n",
    "    - [Why Should I Use A Decision Tree?](#why)\n",
    "    - [Why Shouldn't I Use A Decision Tree?](#not)\n",
    "- [How Does It Work?](#how)\n",
    "    - [A Familiar Example](#example)\n",
    "- [Ensembling Methods](#ensemble)\n",
    "\t- [Bagging](#bagging)\n",
    "\t- [Random Forests](#forests)\n",
    "\t- [Boosting](#boosting)\n",
    "- [Pros and Cons](#pros)\n",
    "- [Lesson Review](#review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import toolkits\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "iris = load_iris()\n",
    "test_idx = [0,50,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"where\"></a>\n",
    "## Where Are We Now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./assets/ml_algo_map.png' style=\"float: center; height: 500px\">\n",
    "\n",
    "Source: [machinelearningmastery.com](http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"recursion\"></a>\n",
    "## Recursion Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A recursion problem...\n",
    "<img src='https://imgs.xkcd.com/comics/fixing_problems.png' style=\"float: center; height: 250px\">\n",
    "\n",
    "Image source: [xkcd.com](https://imgs.xkcd.com/comics/fixing_problems.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Another recursion problem...\n",
    "<img src='http://wiki.secretgeek.net/Image/3d_printer_factory.jpg' style=\"float: center; height: 250px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Yet another recursion problem...\n",
    "The most common example people use to describe [recursion](https://en.wikipedia.org/wiki/Recursion) involves the [factorial function](https://en.wikipedia.org/wiki/Factorial). \n",
    "\n",
    "$$5! = 5*4*3*2*1 = 120$$\n",
    "\n",
    "Imagine struggling to create a custom function to do this with any positive number.\n",
    "- As we can see below, we can implement the factorial in Python *easily* and *elegantly* using recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorial(n):\n",
    "    #print(n)\n",
    "    if n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)\n",
    "    \n",
    "factorial(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In general, recursion can be pretty opaque. But, we can get a clearer picture of what's happening inside our function from the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorial(4)                   # = 24\n",
    "4 * factorial(3)               # = 24\n",
    "4 * 3 * factorial(2)           # = 24\n",
    "4 * 3 * 2 * factorial(1)       # = 24\n",
    "4 * 3 * 2 * 1                  # = 24\n",
    "4 * 3 * 2                      # = 24\n",
    "4 * 6                          # = 24\n",
    "24                             # = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pros:**\n",
    "- Recursive functions make the code look neat and clean.\n",
    "- A complex task can be performed more elegantly by breaking it down into more manageable parts using recursion.\n",
    "\n",
    "**Cons:**\n",
    "- Following the logic behind recursion can be challenging. As a result, recursive functions are often hard to debug.\n",
    "- Recursive calls can be computationally expensive (inefficient) as they can take up a lot of space in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"intuition\"></a>\n",
    "## Intuition Behind Decision Trees\n",
    "\n",
    "**Objective:** Understand the specific function of the decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Decision trees are like the game “_20 questions_” or \"_I Spy_\" -\n",
    "\n",
    "- They make decision by answering a series of questions, most often binary questions (yes or no).\n",
    "\n",
    "- We want the smallest set of questions to get to the right answer.\n",
    "\n",
    "- Each questions should reduce the search space as much as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='./assets/dec_tree_ex_simple.png' style=\"float: center; height: 300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We've all been exposed to tree diagrams before, since they are used widely as a tool for representing potentially complex, decision-like processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg' style=\"float: center; height: 650px\">\n",
    "\n",
    "Image source: [nyt.com](https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this notebook, we'll see how we can use _tree-based methods_ for regression and classification, by applying it to problems similar to those we've previously used linear methods to solve. \n",
    "> As with other models, decision trees are just another way of _stratifying_ or _segmenting_ a given predictor space into (_hopefully_) 'meaningful' groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='why'></a>\n",
    "\n",
    "### Why Should I Use A Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1. Non-linearity\n",
    "\n",
    "When dealing with _non-linear_ data (and we usually are), non-linear decision trees models can provide a limited advantage over logistic regression.\n",
    "\n",
    "> **Reminder:** A '_linear model_' is one in which a change in an input variable has a constant change on the output variable.\n",
    "><img src='./assets/non-linearity.png' style=\"float: center; height: 175px\">\n",
    ">Image source: [erinshellman.github](http://erinshellman.github.io/data-mining-starter-kit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 2. Sensitive to interactions b/t variables\n",
    "Trees ***automatically contain interaction of features***, and so are capable of more complex representations than is possible with linear models alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3. Interpretability\n",
    "\n",
    "The real benefit of decision tree models are their *interpretability*. They elegantly provide intuitive insights into the decision-making process of a given model.\n",
    "- As we've seen, neural networks, SVM’s, etc. ≈ black box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This makes trees very useful for **feature selection***, which can be have a great impact on the accuracy and efficiency of your model.\n",
    "\n",
    "> ***Reminder**: \"*Feature selection*\" is the process of determining the best set of features to include in our model. \n",
    "> - The 'best' features are the ones that create the greatest segregation in the data, in terms of the true label of each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "They helps us ask: “Which of my features are the most helpful in making predictions about my target variable?”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 4. Availability/Ease of use\n",
    "It's also included in most ML libraries (including scikit-learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='not'></a>\n",
    "\n",
    "### Why *Shouldn't* I Use A Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1. Braun (or lack-of)\n",
    "\n",
    "As useful as tree-based methods are for _interpretation_, they typically are considered 'weak model's and are **not competitive*** with the best supervised learning approaches available today.\n",
    "\n",
    "\\*We'll see some solutions to this problem later in the [*Ensembling*](#ensemble) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2. Over-fitting\n",
    "\n",
    "Decision trees tend to be weak models because they can easily memorize or overfit to a dataset.\n",
    "- The algorithm will always find _some_ solution.\n",
    "\n",
    "<img src='./assets/over-fit.png' style=\"float: center; height: 300px\">\n",
    "\n",
    "A model is overfit when it memorizes or bends to a few specific data points rather than picking up general trends in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An unconstrained decision tree can learn an extreme tree (e.g. one feature for each word in a news article) -- as evidence, just take a look at the example below.\n",
    "\n",
    "<img src='./assets/tree.png' style=\"float: center; height: 200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Fortunately, we have a variety of '*pruning methods*' available for us to limit the size of our decision trees, which are described in the next section.\n",
    "\n",
    "> Get it? *Pruning*? Yay for puns!  \n",
    "> <img src='https://waterfrontgardens.org/wp-content/uploads/2019/02/pruning.jpg' style=\"float: left; height: 100px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='how'></a>\n",
    "\n",
    "## How Does It Work?\n",
    "\n",
    "**Objective:** Learn the basic components of the tree-based model and how they are generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Perhaps not surprisingly, *trees* are data structures that are made up of '***leaves***' and '***branches***'.\n",
    "\n",
    "- The point at which each branch splits is called a '***node***'. Typically, each node has two or more branches that connect it to its '*children*' (see figure below).\n",
    "\n",
    "- Each child is another node in the tree and contains its own subtree (meaning many nodes are simultaneously parent and child nodes).\n",
    "\n",
    "- Nodes without any children (terminal nodes) are known as '*leaves*', while the 'branches' correspond to the specific set of rules used by a particular tree to segment the predictor space.\n",
    "\n",
    "<img src='./assets/tree_nodes.png' style=\"float: center; height: 300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **Crucial concept**: *Order matters!* \n",
    "> - The next question is always dependent on the last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Effectively, a decision tree contains a question at every node. This is where [*recursion*](#recursion) comes in handy...\n",
    "\n",
    "- **Step 1**: Evaluate question, depending on answer, proceed down the left or right branch of the tree.\n",
    "- **Step 2**: If we don’t have any more questions (at the leaf nodes), make a prediction*, else repeat Step 1.\n",
    "\n",
    "*A 'prediction', in this case, is just the mean of the response values for the training observations in a given leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Imagine** you wanted to design a model to **categorize** text documents into 'news about the weather' or '*not* news about the weather'.\n",
    "- What questions might we want to ask to make this prediction accurately?\n",
    "- How many questions should we ask?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **For example**, we might ask:  \n",
    ">    Does it mentions the word 'precipitation'?  \n",
    ">    If yes, then we predict 'yes'. If not we ask if it mentions 'temperature', etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But this works for **regression**, too. \n",
    "- We could also imagine designing a model to predict a given baseball player's salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> In this case, we might ask:  \n",
    ">    How many years has the player been with the team? \n",
    "> - Using some cut-off threshold, we decide whether to go left or right from there.  \n",
    "> - Subsequently, we ask how many hits has the player had this year, etc.\n",
    "\n",
    "<img src='./assets/hitters.png' style=\"float: center; height: 500px\">\n",
    "\n",
    "Image source: [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each questions in intended to increase the '*purity*' of the data segments as much as possible.\n",
    "- We can use a variety of metrics to quantify the purity of the separation of groups including: _Classification error_, _Entropy_, or the _**Gini Coefficient**_ (most common).\n",
    "- Using one of these, the model will choose the question that gives us the best change in our purity measure at each step.\n",
    "- This is done *recursively* for each new set of two groups until we reach a stopping point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> **Pro-tip**: \n",
    "> - Sometimes branches are of uniform length.\n",
    "> - If not, their length is usually proportional to the *decrease in impurity*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Potential pitfall**:\n",
    "\n",
    "As mentioned previously, an unconstrained decision tree can become **overfit** very easily.\n",
    "- This can lead to poor test set performance as a result of being too complex.\n",
    "\n",
    "> Although a smaller tree (with fewer splits) might increase the bias cost, they can also lower the variance and provide better interpretations overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fortunately, (as we also mentioned previously) we can limit the size of our decision trees using a few simple '*pruning*' methods:\n",
    "- Limiting the number of questions (nodes) a tree can have.\n",
    "- Limiting the number of samples in the leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> **For example**, we might use a ['_**decision stump**_'](https://en.wikipedia.org/wiki/Decision_stump) model -- a decision tree consisting of just one level.\n",
    "> <img src='https://upload.wikimedia.org/wikipedia/commons/thumb/3/36/Tree_stump1_30u06.JPG/1920px-Tree_stump1_30u06.JPG' style=\"float: center; height: 100px\">\n",
    "> <img src='https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Decision_stump.svg/1200px-Decision_stump.svg.png' style=\"float: center; height: 100px\">\n",
    ">\n",
    "> Makes sense, _right_? We'll see more examples in today's lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='example'></a>\n",
    "\n",
    "### A Familiar Example\n",
    "\n",
    "Decision trees can be applied to both regression and classification problems, but we'll use a simple and familiar dataset for this notebook: the *iris dataset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training/test data\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(iris.data, iris.target, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate and fit model (tree)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# visualize decision tree\n",
    "import graphviz\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)\n",
    "\n",
    "graphviz.Source(dot_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create random index of element in training data\n",
    "rand_select = np.random.randint(0,len(X_train))\n",
    "\n",
    "# let's use the tree to make some predictions\n",
    "print(dict(zip(iris.feature_names, X_train[rand_select])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's double-check our predictions to see if they were right\n",
    "print(iris.target_names[y_train[rand_select]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ensemble'></a>\n",
    "\n",
    "## Ensembling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In statistics and machine learning, '_**ensemble methods**_' use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n",
    "\n",
    "---\n",
    "\n",
    "'*Bagging*', '*random forests*', and '*boosting*' each involve producing multiple trees which are then combined to yield a single prediction (via some form of consensus). \n",
    "- We'll see that combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='bagging'></a>\n",
    "\n",
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping) is a very common statistical technique that allows us to improve on certain statistical-modeling methods such as the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We've already learned that trees can *suffer from high variance*. \n",
    "- Bootstrap aggregation, or '**bagging**', is a *general-purpose procedure* for reducing the variance of a statistical model, which happens to be particularly useful and frequently used in the context of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Bagging** simply leverages the fact that *averaging a set of observations reduces variance*. \n",
    "- Logically, then, one way to reduce variance (and increase prediction accuracy) is to:\n",
    "    1. take many, repeated training sets from the population, \n",
    "    2. build a separate prediction model using each training set, and \n",
    "    3. average the resulting predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Obviously, this isn't usually practical because we don't usually have access to multiple training sets from the population (usually just one).\n",
    "- Fortunately, we can bootstrap, by taking repeated samples from the single training data set (*with replacement*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='forests'></a>\n",
    "### Random Forests\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Biogradska_suma.jpg/440px-Biogradska_suma.jpg' style=\"float: center; height: 150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "'**Random forests**' provide an improvement over bagging in the form of a small tweak that makes the aggregated trees *less correlated with each other*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='https://dsc-spidal.github.io/harp/img/4-5-1.png' style=\"float: center; height: 275px\">\n",
    "\n",
    "Image source: [dsc-spidal.github](https://dsc-spidal.github.io/harp/img/4-5-1.png)\n",
    "- Just like in bagging, we build some number of decision trees on bootstrapped training samples. \n",
    "- But here, each time a split is considered when building a tree, a random sample of\n",
    "$m$ predictors is chosen as split candidates, rather than the full set of $p$ predictors.\n",
    "- As a result, at each split in the tree, the algorithm isn't even allowed to consider a majority of the available predictors, when building the random forest. \n",
    "- Hence the predictions from the random forest trees will be *less correlated* with each other, than the predictions from the bagged trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Random Forest Algorithm  \n",
    "\n",
    "1. Take a bootstrap sample of the dataset   \n",
    "\n",
    "2. Train a decision tree on the bootstrap sample\n",
    "\n",
    "    - For each split/feature selection, only evaluate a limited number of features to find the best one\n",
    "\n",
    "3. Repeat this for N trees  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='pros'></a>\n",
    "\n",
    "### Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Random forest models are one of the most widespread classifiers used.\n",
    "\n",
    "#### Advantages\n",
    "- Easy to tune\n",
    "- Built-in protection against overfitting\n",
    "- Non-linear\n",
    "- Built-in interaction effects\n",
    "\n",
    "#### Disadvantages\n",
    "- Slow\n",
    "- Black-box\n",
    "- No “coefficients” (harder to explain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"topic-review\"></a>\n",
    "## Lesson Review\n",
    "---\n",
    "\n",
    "- We learned about the use of recursion in Python.\n",
    "- We saw how decision trees are built by recursively splitting the data.\n",
    "- We saw that trees can be displayed graphically, and are easily interpreted even bya non-expert (as long as they are small).\n",
    "- We learned about the application of decision tree models to both classification and regression problems.\n",
    "- We saw the limitations of decision tree models, particularly their tendency to become overfit, and learned about 'pruning' methods available to resolve this.\n",
    "- We previewed some of the 'ensembling methods' available to overcome the high-variability inherent in decision tree models.\n",
    "\n",
    "**Any further questions?**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
