{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Decision Trees\n",
    " \n",
    "_Author: Adam Jones, PhD (San Francisco)_\n",
    " \n",
    "---\n",
    "\n",
    "Adapted from numerous sources, including: \n",
    "- [*An Introduction to Statistical Learning*](http://www-bcf.usc.edu/~gareth/ISL/) - Chapter 8\n",
    "\n",
    "\n",
    "\n",
    ">This notebook will rely on Numpy, Matplotlib, and GraphViz.*\n",
    ">\n",
    ">\\* For 'simple' installation: `$ pip install graphviz` \n",
    ">\n",
    ">    [Else, try `$ brew install graphviz` (for mac)]\n",
    ">\n",
    ">For more info, see: https://pypi.org/project/graphviz/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "## Learning Objectives\n",
    "\n",
    "- Identify the types of problems a neural network is likely to succeed at.\n",
    "- Explain the similarities/differences between real and artificial neurons.\n",
    "- Define the working components of a basic, feed-forward neural network.\n",
    "- Determine how to apply neural networks to the prediction problem of your choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Where Are We Now?](#where-are-we-now)\n",
    "- [Recursion Example](#recursion)\n",
    "- [Intuition Behind Decision Trees](#intuition)\n",
    "    - [Why Should I Use A Decision Tree?](#why)\n",
    "    - [Why Shouldn't I Use A Decision Tree?](#why-not)\n",
    "- [How Does It Work?](#how)\n",
    "    - [An Example](#example)\n",
    "- [Ensembling Methods](#ensemble)\n",
    "\t- [Bagging](#bagging)\n",
    "\t- [Random Forests](#forests)\n",
    "\t- [Boosting](#boosting)\n",
    "- [Lesson Review](#topic-review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import toolkits\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataset\n",
    "iris = load_iris()\n",
    "test_idx = [0,50,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"where-are-we-now\"></a>\n",
    "## Where Are We Now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='../assets/ml_algo_map.png' style=\"float: center; height: 500px\">\n",
    "Source: [machinelearningmastery.com](http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"recursion\"></a>\n",
    "## Recursion Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://imgs.xkcd.com/comics/fixing_problems.png' style=\"float: center; height: 250px\">\n",
    "Image source: [xkcd.com](https://imgs.xkcd.com/comics/fixing_problems.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most common example people use to describe [recursion](https://en.wikipedia.org/wiki/Recursion) involves the [factorial function](https://en.wikipedia.org/wiki/Factorial). \n",
    "\n",
    "$$5! = 5*4*3*2*1 = 120$$\n",
    "\n",
    "Imagine struggling to create a custom function to do this with any positive number. As we can see below, we can implement the factorial in Python easily and elegantly using recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def factorial(n):\n",
    "    #print(n)\n",
    "    if n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)\n",
    "    \n",
    "factorial(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In general, recursion can be pretty opaque. But, we can get a clearer picture of what's happening inside our function from the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factorial(4)                   # = 24\n",
    "4 * factorial(3)               # = 24\n",
    "4 * 3 * factorial(2)           # = 24\n",
    "4 * 3 * 2 * factorial(1)       # = 24\n",
    "4 * 3 * 2 * 1                  # = 24\n",
    "4 * 3 * 2                      # = 24\n",
    "4 * 6                          # = 24\n",
    "24                             # = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pros:**\n",
    "- Recursive functions make the code look neat and clean.\n",
    "- A complex task can be performed more elegantly by breaking it down into more manageable parts using recursion.\n",
    "\n",
    "**Cons:**\n",
    "- Following the logic behind recursion can be challenging. As a result, recursive functions are often hard to debug.\n",
    "- Recursive calls can be computationally expensive (inefficient) as they can take up a lot of space in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"intuition\"></a>\n",
    "## Intuition Behind Decision Trees\n",
    "\n",
    "**Objective:** Understand the specific function of the decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We've all been exposed to tree diagrams before, since they are used widely as a tool for representing potentially complex, decision-like processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg' style=\"float: center; height: 500px\">\n",
    "Image source: [nyt.com](https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Decision trees are like the game “20 questions”-\n",
    "\n",
    "- They make decision by answering a series of questions, most often binary questions (yes or no).\n",
    "\n",
    "- We want the smallest set of questions to get to the right answer.\n",
    "\n",
    "- Each questions should reduce the search space as much as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='../assets/dec_tree_ex_simple.png' style=\"float: center; height: 300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this notebook, we'll see how we can use _tree-based methods_ for regression and classification, by applying it to problems similar to those we've previously used linear methods to solve. As with other models, decision trees are just another way of _stratifying_ or _segmenting_ a given predictor space into 'meaningful' (_hopefully_) groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='why'></a>\n",
    "\n",
    "### Why Should I Use A Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Non-linearity**\n",
    "\n",
    "When dealing with _non-linear_ data (and we usually are), non-linear decision trees models can provide a limited advantage over logistic regression.\n",
    "\n",
    "> A '_linear model_' is one in which a change in an input variable has a constant change on the output variable.\n",
    "><img src='../assets/non-linearity.png' style=\"float: center; height: 175px\">\n",
    ">Image source: [erinshellman.github](http://erinshellman.github.io/data-mining-starter-kit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Trees automatically contain interaction of features, and so are capable of more complex representations than is possible with linear models alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Interpretability**\n",
    "\n",
    "The real benefit of decision tree models are their *interpretability*. They elegantly provide intuitive insights into the decision-making process of a given model.\n",
    "- As we've seen, neural networks, SVM’s, etc. ≈ black box\n",
    "\n",
    "This makes them very useful for **feature selection***, which can be have a great impact on the accuracy and efficiency of your model.\n",
    "\n",
    ">\"Feature selection\" is the process of determining the best set of features to include in our model. The 'best' features are the ones that create the greatest segregation in the data, in terms of the true label of each observation.\n",
    "\n",
    "They helps us ask: “Which of my features are the most helpful in making predictions about my target variable?”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Bonus**: It's also included in most ML libraries (including scikit-learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='why-not'></a>\n",
    "\n",
    "### Why *Shouldn't* I Use A Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Braun (or lack-of)**\n",
    "\n",
    "As useful as tree-based methods are for _interpretation_, they typically are considered 'weak model's and are **not competitive*** with the best supervised learning approaches available today.\n",
    "*See [#ensemble] for some solutions to this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Over-fitting**\n",
    "\n",
    "Decision trees tend to be weak models because they can easily memorize or overfit to a dataset.\n",
    "- The algorithm will always find _some_ solution\n",
    "\n",
    "<img src='../assets/over-fit.png' style=\"float: center; height: 300px\">\n",
    "\n",
    "A model is overfit when it memorizes or bends to a few specific data points rather than picking up general trends in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An unconstrained decision tree can learn an extreme tree (e.g. one feature for each word in a news article) -- see example below.\n",
    "\n",
    "<img src='./practice/tree.png' style=\"float: center; height: 300px\">\n",
    "\n",
    "Fortunately, we can limit our decision trees using 'pruning' methods, which are described in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='how'></a>\n",
    "\n",
    "## How Does It Work?\n",
    "\n",
    "**Objective:** Learn the basic components of the tree-based model and how they are generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Perhaps not surprisingly, trees are data structures that are made up of '*leaves*' and '*branches*'.\n",
    "\n",
    "- The point at which each branch splits is called a '*node*'. Typically, each node has two or more branches that connect it to its '*children*' (see figure below).\n",
    "\n",
    "- Each child is another node in the tree and contains its own subtree (meaning many nodes are simultaneously parent and child nodes).\n",
    "\n",
    "- Nodes without any children (terminal nodes) are known as 'leaves', while the 'branches' correspond to the specific set of rules used by a particular tree to segment the predictor space.\n",
    "\n",
    "<img src='../assets/tree_nodes.png' style=\"float: center; height: 300px\">\n",
    "\n",
    "*Note*: The next question is always dependent on the last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Effectively, a decision tree contains a question at every node. This is where recursion comes in handy...\n",
    "\n",
    "- **Step 1**: Evaluate question, depending on answer, proceed down the left or right branch of the tree.\n",
    "- **Step 2**: If we don’t have any more questions (at the leaf nodes), make a prediction*, else repeat Step 1.\n",
    "\n",
    "*A 'prediction', in this case, is just the mean of the response values for the training observations in a given leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Imagine** you wanted to design a model to **categorize** text documents into 'news about the weather' or 'not news about the weather'.\n",
    "- What questions might we want to ask to make this prediction accurately?\n",
    "- How many questions should we ask?\n",
    "\n",
    "> **For example**, we might ask:  \n",
    ">    Does it mentions the word 'precipitation'?  \n",
    ">    If yes, then we predict 'yes'. If not we ask if it mentions 'temperature', etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But this works for **regression**, too. We could also imagine designing a model to predict a given baseball player's salary.\n",
    "    \n",
    "> In this case, we might ask:  \n",
    ">    How many hits has the player has this year? Using some cut-off threshold, we decide whether to go left or right from there.  \n",
    ">    Subsequently, we ask how many years has the player been on the team, etc.\n",
    "\n",
    "<img src='../assets/hitters.png' style=\"float: center; height: 500px\">\n",
    "Image source: [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each questions in intended to increase the '*purity*' of the data segments as much as possible.\n",
    "- We can use a variety of metrics to quantify the purity of the separation of groups including: Classification error, Entropy, or the Gini Coefficient.\n",
    "- Using one of these, the model will choose the question that gives us the best change in our purity measure at each step.\n",
    "- This is done *recursively* for each new set of two groups until we reach a stopping point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As mentioned previously, an unconstrained decision tree can become **overfit** very easily. This can lead to poor test set performance as a result of being too complex.\n",
    "\n",
    "Although a smaller tree (with fewer splits) might increase the bias cost, they can also lower the variance and provide better interpretations overall.\n",
    "\n",
    "Fortunately, we can limit the size of our decision trees using a few simple '*pruning*' methods:\n",
    "- Limiting the number of questions (nodes) a tree can have.\n",
    "- Limiting the number of samples in the leaf nodes.\n",
    "\n",
    "We'll see how they are employed in today's lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='example'></a>\n",
    "\n",
    "### An Example\n",
    "\n",
    "Decision trees can be applied to both regression and classification problems, but we'll use a simple and familiar dataset for this notebook: the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split training/test data\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(iris.data, iris.target, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tree instantiation\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8947368421052632"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"358pt\" height=\"552pt\"\n",
       " viewBox=\"0.00 0.00 358.00 552.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 548)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-548 354,-548 354,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.027451\" stroke=\"black\" d=\"M201,-544C201,-544 78,-544 78,-544 72,-544 66,-538 66,-532 66,-532 66,-473 66,-473 66,-467 72,-461 78,-461 78,-461 201,-461 201,-461 207,-461 213,-467 213,-473 213,-473 213,-532 213,-532 213,-538 207,-544 201,-544\"/>\n",
       "<text text-anchor=\"start\" x=\"74\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 0.8</text>\n",
       "<text text-anchor=\"start\" x=\"104\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.666</text>\n",
       "<text text-anchor=\"start\" x=\"94.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 112</text>\n",
       "<text text-anchor=\"start\" x=\"81.5\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [39, 36, 37]</text>\n",
       "<text text-anchor=\"start\" x=\"96\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M111,-417.5C111,-417.5 18,-417.5 18,-417.5 12,-417.5 6,-411.5 6,-405.5 6,-405.5 6,-361.5 6,-361.5 6,-355.5 12,-349.5 18,-349.5 18,-349.5 111,-349.5 111,-349.5 117,-349.5 123,-355.5 123,-361.5 123,-361.5 123,-405.5 123,-405.5 123,-411.5 117,-417.5 111,-417.5\"/>\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"23.5\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 39</text>\n",
       "<text text-anchor=\"start\" x=\"14\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [39, 0, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"21\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M113.479,-460.907C106.264,-449.652 98.422,-437.418 91.1706,-426.106\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"94.1041,-424.197 85.7609,-417.667 88.211,-427.975 94.1041,-424.197\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.4184\" y=\"-438.389\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.027451\" stroke=\"black\" d=\"M276,-425C276,-425 153,-425 153,-425 147,-425 141,-419 141,-413 141,-413 141,-354 141,-354 141,-348 147,-342 153,-342 153,-342 276,-342 276,-342 282,-342 288,-348 288,-354 288,-354 288,-413 288,-413 288,-419 282,-425 276,-425\"/>\n",
       "<text text-anchor=\"start\" x=\"149\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.7</text>\n",
       "<text text-anchor=\"start\" x=\"186.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"173.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 73</text>\n",
       "<text text-anchor=\"start\" x=\"160\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 37]</text>\n",
       "<text text-anchor=\"start\" x=\"166\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M165.521,-460.907C171.164,-452.105 177.19,-442.703 183.018,-433.612\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"186.075,-435.328 188.525,-425.021 180.182,-431.551 186.075,-435.328\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.868\" y=\"-445.743\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.945098\" stroke=\"black\" d=\"M202.5,-306C202.5,-306 74.5,-306 74.5,-306 68.5,-306 62.5,-300 62.5,-294 62.5,-294 62.5,-235 62.5,-235 62.5,-229 68.5,-223 74.5,-223 74.5,-223 202.5,-223 202.5,-223 208.5,-223 214.5,-229 214.5,-235 214.5,-235 214.5,-294 214.5,-294 214.5,-300 208.5,-306 202.5,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"70.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.0</text>\n",
       "<text text-anchor=\"start\" x=\"110.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.1</text>\n",
       "<text text-anchor=\"start\" x=\"97.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 38</text>\n",
       "<text text-anchor=\"start\" x=\"88\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"86\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M188.132,-341.907C182.414,-333.105 176.307,-323.703 170.402,-314.612\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"173.204,-312.5 164.821,-306.021 167.333,-316.313 173.204,-312.5\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M338,-298.5C338,-298.5 245,-298.5 245,-298.5 239,-298.5 233,-292.5 233,-286.5 233,-286.5 233,-242.5 233,-242.5 233,-236.5 239,-230.5 245,-230.5 245,-230.5 338,-230.5 338,-230.5 344,-230.5 350,-236.5 350,-242.5 350,-242.5 350,-286.5 350,-286.5 350,-292.5 344,-298.5 338,-298.5\"/>\n",
       "<text text-anchor=\"start\" x=\"263.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"250.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 35</text>\n",
       "<text text-anchor=\"start\" x=\"241\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 35]</text>\n",
       "<text text-anchor=\"start\" x=\"243\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>2&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M241.215,-341.907C248.622,-330.652 256.673,-318.418 264.118,-307.106\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"267.098,-308.944 269.672,-298.667 261.251,-305.096 267.098,-308.944\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-179.5C109,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 109,-111.5 109,-111.5 115,-111.5 121,-117.5 121,-123.5 121,-123.5 121,-167.5 121,-167.5 121,-173.5 115,-179.5 109,-179.5\"/>\n",
       "<text text-anchor=\"start\" x=\"32.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"19.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 35</text>\n",
       "<text text-anchor=\"start\" x=\"10\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 35, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M111.438,-222.907C103.935,-211.652 95.7789,-199.418 88.2375,-188.106\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"91.0705,-186.046 82.6113,-179.667 85.2462,-189.929 91.0705,-186.046\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.498039\" stroke=\"black\" d=\"M282,-187C282,-187 151,-187 151,-187 145,-187 139,-181 139,-175 139,-175 139,-116 139,-116 139,-110 145,-104 151,-104 151,-104 282,-104 282,-104 288,-104 294,-110 294,-116 294,-116 294,-175 294,-175 294,-181 288,-187 282,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"147\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.55</text>\n",
       "<text text-anchor=\"start\" x=\"181\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"179\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"169.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"168\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M165.562,-222.907C171.43,-214.105 177.698,-204.703 183.759,-195.612\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"186.851,-197.283 189.486,-187.021 181.027,-193.4 186.851,-197.283\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M193,-68C193,-68 104,-68 104,-68 98,-68 92,-62 92,-56 92,-56 92,-12 92,-12 92,-6 98,-0 104,-0 104,-0 193,-0 193,-0 199,-0 205,-6 205,-12 205,-12 205,-56 205,-56 205,-62 199,-68 193,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"111\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"101.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"100\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M191.179,-103.726C185.742,-94.9703 179.987,-85.7032 174.523,-76.9051\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"177.428,-74.9484 169.179,-68.2996 171.481,-78.6413 177.428,-74.9484\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\n",
       "<path fill=\"#39e581\" stroke=\"black\" d=\"M332,-68C332,-68 235,-68 235,-68 229,-68 223,-62 223,-56 223,-56 223,-12 223,-12 223,-6 229,-0 235,-0 235,-0 332,-0 332,-0 338,-0 344,-6 344,-12 344,-12 344,-56 344,-56 344,-62 338,-68 332,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"255.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"246\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"236.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"231\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M241.448,-103.726C246.806,-94.9703 252.476,-85.7032 257.859,-76.9051\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"260.891,-78.6563 263.125,-68.2996 254.92,-75.0028 260.891,-78.6563\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f1ca6db2e10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Visualize decision tree\n",
    "import graphviz\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)\n",
    "\n",
    "graphviz.Source(dot_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sepal length (cm)': 5.0, 'sepal width (cm)': 3.0, 'petal length (cm)': 1.6, 'petal width (cm)': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# Create random index of element in training data\n",
    "rand_select = np.random.randint(0,len(X_train))\n",
    "\n",
    "# Let's use the tree to make some predictions\n",
    "print(dict(zip(iris.feature_names, X_train[rand_select])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa\n"
     ]
    }
   ],
   "source": [
    "# Let's double-check our predictions to see if they were right.\n",
    "print(iris.target_names[y_train[rand_select]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ensemble'></a>\n",
    "\n",
    "## Ensembling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics and machine learning, '*ensemble methods*' use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n",
    "\n",
    "'*Bagging*', '*random forests*', and '*boosting*' each involve producing multiple trees which are then combined to yield a single consensus prediction. We'll see that combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='bagging'></a>\n",
    "\n",
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping) is a very common statistical technique that allows us to improve on certain statistical-modeling methods such as the decision tree.\n",
    "\n",
    "We've already learned that trees can suffer from high variance. Bootstrap aggregation, or '*bagging*', is a general-purpose procedure for reducing the variance of a statistical model, which happens to be particularly useful and frequently used in the context of decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This method simply leverages the fact that *averaging a set of observations reduces variance*. Logically, then, one way to reduce variance (and increase prediction accuracy) is to take many, repeated training sets from the population, build a separate prediction model using each training set, and average the resulting predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Obviously, this isn't practical because we don't usually have access to multiple training sets from the population. Fortunately, we can bootstrap, by taking repeated samples from the single training data set (*with replacement*). This is called '*bagging*'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='forests'></a>\n",
    "\n",
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='https://dsc-spidal.github.io/harp/img/4-5-1.png' style=\"float: center; height: 275px\">\n",
    "\n",
    "Image source: [dsc-spidal.github](https://dsc-spidal.github.io/harp/img/4-5-1.png)\n",
    "\n",
    "Random forests provide an improvement over bagged trees in the form of a small tweak that makes the aggregated trees less correlated with each other. Just like in bagging, we build some number of decision trees on bootstrapped training samples. But here, each time a split is considered when building a tree, a random sample of\n",
    "$m$ predictors is chosen as split candidates, rather than the full set of $p$ predictors.\n",
    "\n",
    "As a result, at each split in the tree, the algorithm is not even allowed to consider a majority of the available\n",
    "predictors, when building the random forest. Hence the predictions from the random forest trees will be less correlated with each other, than the predictions from the bagged trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Random Forest Algorithm  \n",
    "\n",
    "1. Take a bootstrap sample of the dataset   \n",
    "\n",
    "2. Train a decision tree on the bootstrap sample\n",
    "\n",
    "    - For each split/feature selection, only evaluate a limited number of features to find the best one\n",
    "\n",
    "3. Repeat this for N trees  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Random forest models are one of the most widespread classifiers used.\n",
    "\n",
    "#### Advantages\n",
    "- Easy to tune\n",
    "- Built-in protection against overfitting\n",
    "- Non-linear\n",
    "- Built-in interaction effects\n",
    "\n",
    "#### Disadvantages\n",
    "- Slow\n",
    "- Black-box\n",
    "- No “coefficients” (harder to explain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"topic-review\"></a>\n",
    "## Lesson Review\n",
    "---\n",
    "\n",
    "- We learned about the use of recursion in Python.\n",
    "- We saw how decision trees are built by recursively splitting the data.\n",
    "- We saw that trees can be displayed graphically, and are easily interpreted even bya non-expert (as long as they are small).\n",
    "- We learned about the application of decision tree models to both classification and regression problems.\n",
    "- We saw the limitations of decision tree models, particularly their tendency to become overfit, and learned about 'pruning' methods available to resolve this.\n",
    "- We previewed some of the 'ensembling methods' available to overcome the high-variability inherent in decision tree models.\n",
    "\n",
    "**Any further questions?**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
