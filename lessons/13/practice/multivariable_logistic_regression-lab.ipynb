{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Multi-Variable Logistic Regression and Classification Matrix\n",
    "\n",
    "_Authors: Sam Stack(DC)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise Objectives**\n",
    "- Hand on experience using Multi-Variable Logistic Regression\n",
    "- Review and Exploration of the Classification Matrix and its evaluation Metrics\n",
    "- Introduction to One vs. One and One vs. Rest Classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Lets get some data.**\n",
    "One of the most popular classification datasets for Machine learning is the Iris Dataset, which can be loaded directly from `sklearn.datasets`\n",
    "- Sklearn datasets are imported as dictionaries and use keys to access specific aspects.\n",
    "    - `iris.data` : actual matrix of observations\n",
    "    - `iris.target` : target column for classification\n",
    "    - `iris.feature_names` :  column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = \n",
    "y ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Break down of classes**  \n",
    "0 : Setosa  \n",
    "1 : Versicolour  \n",
    "2 : Virginica  \n",
    "\n",
    "----\n",
    "\n",
    "**Modelling**\n",
    "This data is extreamly neat and tidy so no cleaning necessary and we can get right into modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model the data, use a cross validation technique as well\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluated model preformance with a confusion matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With a multivariable confusion matrix, some of our labellings (True Pos., True Neg., False Pos., False Neg.) get a little warped.  We are no longer predicting one class from a null class we are classifying into 3 distinguished classes.  \n",
    "\n",
    "The **True** diagonal stays the same as these are properly classified observations.  \n",
    "\n",
    "\n",
    "|     | Class 0 | Class 1  | Class 2 |\n",
    "| --- | ------- |:--------:| -------:|\n",
    "| **Pred Class 0**  | 15      | 0        | 0       |\n",
    "| **Pred Class 1**    | 0       | 11       |   0     |\n",
    "| **Pred Class 2**    | 0       | 1        |    11   |\n",
    "\n",
    "\n",
    "It is better to stick with True and False labels with multi-class to avoid ...[_Confusion_](https://www.youtube.com/watch?v=bcYppAs6ZdI)\n",
    "\n",
    "If you need to refer to a False Positive or True Negative it is better to first select a specific class, such as `Class 2 ` and refer to classification or misclassification relative to said chosen class instead of the set of all classes as a whole. \n",
    "\n",
    "Example:\n",
    "    _True Negatives relative to Class 2 are True Positives for Class 0 and Class 1._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Speaking of our Classes?  How are probabilities calculated with multi class?\n",
    "- Are they Probability of `Class 0` vs. `Not Class 0`?\n",
    "- Or Probability of `Class 0` vs. `Class 1` vs. `Class 2` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use predict_proba to find out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looks like our probabilities of each class all add up to 1, so it is like `Class 0` vs. `Class 1` vs. `Class 2`.\n",
    "\n",
    "What if we wanted to create a logistic regression that has `Class 0` vs. `Class 1` & `Class 2` or just `Class 0` vs. `Class 2`?  We will cover that in a bit, but first more evaluation metrics.\n",
    "\n",
    "---\n",
    "\n",
    "**Classification Reports/Matrix**\n",
    "\n",
    "Classification reports are another means of evaluation classification models and return a few metrics that are based on True Positives, False Positives and False Negatives.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Precision**  \n",
    "- \"How many of the items selected are relevant.\"\n",
    "- Of the items placed into a class, how many of the are True Positives.\n",
    "\n",
    "\n",
    "$$\\frac{True Positives}{True Positives + False Positives}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Recall**  \n",
    "- \"How many of the relevant items are selected.\"\n",
    "- Of the items that were suppose to be placed into a class, how many did we accurately place.\n",
    "\n",
    "\n",
    "$$\\frac{True Positives}{True Positives + False Negatives}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**F1-Score**\n",
    "\n",
    "F1 exists on a range of 0 - 1 where 0 is just awful and 1 is perfect.\n",
    "F1 is considered a harmonic mean as it averages Precision and Recall.  With classification models you often times have to choose what kind of error you are willing to increase in order to reduce the other and thus you may want to optimize Precision or Recall accordingly.  If you are uncertain which you should optimize, F1 score may be the metric of choice.\n",
    "\n",
    "$$2*\\frac{precision * recall}{precision + recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Support**\n",
    "Number of true observations in given class.  The count of possible true observations.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro to Ensembling\n",
    "\n",
    "Earlier we talked about building models relative to class combinations.  Distinguishing One class from all other classes or just One specific class from another specific class.  These goals are possible with Logistic Regression.\n",
    "\n",
    "Up until this point we have used one model, but there are also Machine Learning methods that involve combining several models to come to a more refined conclusion, commonly referred to as Ensemble Methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Vs. Rest Classification.\n",
    "\n",
    "One vs. Rest Classification is a method that builds an individual model for each class to try to distinguish said specific class from the rest of the classes.  Since we are only focusing on one class, `Class 1` these classifiers will group `Class2`, `Class3`, `Class4` into a single class of `Not Class 1`.  Same all the way through for the rest of the classes.\n",
    "\n",
    "1 - Class1 vs. Class2, Class3, Class4  \n",
    "2 - Class2 vs. Class1, Class3, Class4  \n",
    "3 - Class3 vs. Class1, Class2, Class4   \n",
    "4 - Class4 vs. Class1, Class2, Class3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Vs. One Classification.\n",
    "\n",
    "We train a model for every set of classes.  As more classes are added this becomes more computationally expense.  \n",
    "\n",
    "1 - Class1 vs. Class2  \n",
    "2 - Class1 vs. Class3  \n",
    "3 - Class1 vs. Class4  \n",
    "4 - Class2 vs. Class3  \n",
    "5 - Class2 vs. Class4  \n",
    "6 - Class3 vs. Class4  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Vs. Rest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ensemble method\n",
    "from sklearn.multiclass import OneVsRestClassifier \n",
    "# instantiate choose model\n",
    "LR = LogisticRegression()\n",
    "# place the model in the ensembler\n",
    "OVC = OneVsRestClassifier(LR)\n",
    "# use the ensemble like a normal sklearn model\n",
    "OVC.fit(x_train, y_train)\n",
    "\n",
    "# you can use the train test split you created earlier or do a new TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the .predict and confusion matrix the same way\n",
    "y_pred = OVC.predict(x_test)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Vs. One Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "LR = LogisticRegression()\n",
    "# OvO works the same as OvR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction and evaluate confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Vs. One/Rest Classifiers are not restricted to fitting using Logistic Regression.  With SKLearn, any type of Classification model can be placed into the One Vs X classification ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ROC Curves\n",
    "[Source1](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)\n",
    "\n",
    "[Source2](https://www.medcalc.org/manual/roc-curves.php)\n",
    "\n",
    "When you consider the results of a particular test in two populations, one population with a disease, the other population without the disease, you will rarely observe a perfect separation between the two groups. Indeed, the distribution of the test results will overlap, as shown in the following figure.\n",
    "\n",
    "<img src=\"https://www.medcalc.org/manual/_help/images/roc_intro1.png\" style=\"width: 350px;\"/>\n",
    "\n",
    "For every possible cut-off point or criterion value you select to discriminate between the two populations, there will be some cases with the disease correctly classified as positive (TP = True Positive fraction), but some cases with the disease will be classified negative (FN = False Negative fraction). On the other hand, some cases without the disease will be correctly classified as negative (TN = True Negative fraction), but some cases without the disease will be classified as positive (FP = False Positive fraction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$True Positive Rate = TP/Positive = 1 - False Negative Rate$$\n",
    "\n",
    "$$False Postive Rate = FP/Negative = 1 - True Negative Rate$$\n",
    "\n",
    "The ROC curve is a useful tool for a few reasons:\n",
    "\n",
    "- The curves of different models can be compared directly in general or for different thresholds.\n",
    "- The area under the curve (AUC) can be used as a summary of the model skill.\n",
    "- The shape of the curve contains a lot of information, including what we might care about most for a problem, the expected false positive rate, and the false negative rate.\n",
    "\n",
    ">The AUC for the ROC can be calculated using the `.roc_auc_score()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A skillful model will assign a higher probability to a randomly chosen real positive occurrence than a negative occurrence on average. This is what we mean when we say that the model has skill. Generally, skillful models are represented by curves that bow up to the top left of the plot.\n",
    "\n",
    "A model with no skill is represented at the point [0.5, 0.5]. A model with no skill at each threshold is represented by a diagonal line from the bottom left of the plot to the top right and has an AUC of 0.0.\n",
    "\n",
    "A model with perfect skill is represented at a point [0.0 ,1.0]. A model with perfect skill is represented by a line that travels from the bottom left of the plot to the top left and then across the top to the top right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "# Get prediction (probability)\n",
    "probs = lr.predict_proba(x_test)\n",
    "\n",
    "# Plot 'no-skill' level\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "# Iterate through each class\n",
    "for i in range(n_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_test[:, i], probs[:, i])   \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot the roc curve for each class\n",
    "    pyplot.plot(fpr, tpr, marker='.')\n",
    "    pyplot.xlabel('FPR (misses)')\n",
    "    pyplot.ylabel('TPR (hits)')\n",
    "    \n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision-Recall Curves\n",
    "\n",
    "**Precision** and **recall** are useful measures in machine learning for evaluating binary classification models.\n",
    "\n",
    "*Precision* is a ratio of the number of true positives divided by the sum of the true positives and false positives. It describes how good a model is at predicting the positive class. Precision is also referred to as the positive predictive value.\n",
    "\n",
    "$Precision = TP / (TP + FP)$\n",
    "\n",
    "*Recall* is calculated as the ratio of the number of true positives divided by the sum of the true positives and the false negatives. Recall is the same as sensitivity.\n",
    "\n",
    "$Recall = TP / (TP + FN)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What are they for?\n",
    "\n",
    "Both precision and recall are useful in cases where there is an ***imbalance*** in the observations between the two classes. Specifically, there are many examples of no event (class 0) and only a few examples of an event (class 1).\n",
    "\n",
    "The reason for this is that typically the large number of class 0 examples means we are less interested in the skill of the model at predicting class 0 correctly (i.e. high true negatives) and more interested in predicting class 1 correctly (i.e. high true positives).\n",
    "\n",
    ">Key to the calculation of precision and recall is that the calculations do not make use of the true negatives. It is only concerned with the correct prediction of the minority class, class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A precision-recall curve is a plot of the precision (y-axis) and the recall (x-axis) for different thresholds, much like the ROC curve.\n",
    "\n",
    "A model with perfect skill is depicted as a point at [1.0,1.0]. A skillful model is represented by a curve that bows towards [1.0,1.0].\n",
    "\n",
    "<img src=\"https://classeval.files.wordpress.com/2015/06/roc-precision-recall-one-to-one-relationship.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When to Use ROC vs. Precision-Recall Curves?\n",
    "Generally, the use of ROC curves and precision-recall curves are as follows:\n",
    "\n",
    "ROC curves should be used when there are roughly equal numbers of observations for each class.\n",
    "Precision-Recall curves should be used when there is a moderate to large class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "- ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds.\n",
    "- Precision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.\n",
    "- ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
